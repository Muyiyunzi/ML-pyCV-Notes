{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Notes\n",
    "    整理ML:C2模型评估与选择笔记内容，新学习ML:C3线性模型内容。以后的笔记应该还是以“讲解”的视角去做，而不是单纯地整理概念，这样无论是再回看，还是写的过程都会更加有趣一些。\n",
    "  \n",
    "## ML C2 模型评估与选择\n",
    "    本章解决的一个核心问题就是在于，以什么样的指标去选择模型，如何评估这些模型之间的好坏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 经验误差与过拟合\n",
    "* 训练误差、泛化误差\n",
    "  - 误差：学习器预测输出 与 样本真实标记 之间的差异\n",
    "  - 在训练集上的误差叫**训练误差**，在新样本上的误差叫**泛化误差**\n",
    "  - 我们当然希望得到泛化误差小的学习器→但往往只能是使经验误差最小化\n",
    "* 过拟合、欠拟合\n",
    "  - 过拟合就好像书呆子、欠拟合就好像“井底之蛙”\n",
    "![fitting](image/fitting.jpg)\n",
    "  - 所以机械学习其实也可以理解为一个完全的过拟合？但似乎也不太准确hhh\n",
    "  - 机器学习往往面临的都是NP难甚至更难，那么如果想在P层面完全避免过拟合，那么实际上我们就好像证明了P=NP，显然，至少目前这是很困难的一件事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 评估方法\n",
    "核心目标：我们希望学习器的泛化误差\n",
    "<br>大思路：通过实验的方式测试这些泛化误差并选择→使用测试集进行测试\n",
    "<br>而测试集尽量要和训练集互斥：这就好像老师不能用上课讲的10道例题做期末考试题一样\n",
    "<br>所以我们需要将整个数据集划分为训练集S和测试集T，划分的方法主要有：留出法、交叉验证法和自助法\n",
    "\n",
    "#### 2.2.1 留出法\n",
    "核心思想：将数据集划分为两个互斥的集合，其中一个作为训练集S、另一个作为测试集T。\n",
    "* 要尽可能保证数据分布的一致性，常用的方式是“分层采样”(stratified sampling)\n",
    "* 即使划分的比例确定了，也存在多种划分方式：比如哪些正例作为测试集，哪些作为样本集，所以一般需要若干次随机划分、重复进行实验评估再取平均值，这才是留出法得到的评估结果。\n",
    "* 这种方式有一个窘境在于：训练集部分划分的太大或太小——类似于过拟合的问题，所以往往训练集会占到全部样本的$\\frac{2}{3}\\sim\\frac{4}{5}$\n",
    "\n",
    "#### 2.2.2 交叉验证法\n",
    "核心思想：将数据集D划分为k个大小相似的互斥子集，每次用k-1个作为训练集，k个作为测试集，最终返回结果的均值\n",
    "* 因此亦叫k折交叉验证，通常取10，即10折交叉验证，其他常用如5、20等\n",
    "* 同样地，即使确定了划分的k，仍然存在多种划分方式，所以也要做多次，叫做p次k折验证，例如常见的有10次10折交叉验证\n",
    "* 若令k=样本数量，则得到了交叉验证法的一个特例：留一法（Leave-One-Out，简称LOO）\n",
    "* 留一法的评估结果往往被认为是比较准确的（因为使用的样本数量只比原模型少一个）\n",
    "* 但在数据集较大的情况下往往计算开销无法容忍，而且估计结果也未必更准确（NFL）\n",
    "\n",
    "#### 2.2.3 自助法（其实很少用到了）\n",
    "核心思想：保证训练集的大小和D一样（从而不被评估影响），于是相当于将m个样本拓展到2m个\n",
    "* 相当于每次采样都做放回操作，重复m次得到一个大小为m的样本（显然有重复）D'，作为训练集\n",
    "* 大约会有0.368的样本不会被采样到，用这一部分作为测试集。\n",
    "<br>$$\\lim _{m \\rightarrow \\infty}\\left(1-\\frac{1}{m}\\right)^{m} = \\frac{1}{e} \\approx 0.368$$\n",
    "* 这样的测试结果亦称包外估计（out-of-bag estimate）\n",
    "* m不必很大就可以接近0.37。所以常用于数据集较小、难以有效划分的情况。（所以大数据时代也不咋用了）\n",
    "* 缺点：改变了数据集的初始分布，引入了估计偏差\n",
    "\n",
    "#### 2.2.4 调参与最终模型\n",
    "调参指对算法参数进行设定，所以说和算法选择区别不大——但是并不是所有参数都可以训练出来，于是就需要手动调参\n",
    "* 常见的做法是，对每个参数设定范围和变化步长，从这几个候选值当中选定值\n",
    "* 当然了，即使这样工作量也还是很大的\n",
    "最终模型：在选定模型之后，需要用数据集重新进行训练，再提交给用户\n",
    "* 前边做的都是验证，最后才是交付，交付就要尽可能多用到数据\n",
    "* 也有学者使用**验证集（validation set）**的概念用做模型评估与选择时的测试数据集，与实际使用时的测试集加以区分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 性能度量\n",
    "有了以上方法之后，就需要具体的评估指标来评价学习模型的泛化能力。这种评价标准就是**性能度量(performance measure)**。\n",
    "回归任务最常用的性能度量是**均方误差(MSE, mean square error)**：\n",
    "$$E(f ; D)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2}$$\n",
    "更一般地，对于数据分布$\\mathcal{D}$和概率密度函数$p(·)$，MSE可描述为：\n",
    "$$E(f ; \\mathcal{D})=\\int_{\\boldsymbol{x} \\sim \\mathcal{D}}(f(\\boldsymbol{x})-y)^{2} p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}$$\n",
    "回归的都好说，那么接下来主要考虑**分类任务**中如何衡量性能度量。\n",
    "\n",
    "#### 2.3.1 错误率与精度\n",
    "* 错误率、精度（同样适用于多分类任务）\n",
    "  - 错误率：分类错误的样本数占总数的比率\n",
    "$$E(f ; D)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right)$$\n",
    "  - 精度：无论数据输入离散连续，都是 1-错误率\n",
    "  - 对于离散的数据分布$\\mathcal{D}$和概率密度函数$p(·)$而言，错误率公式为：\n",
    "$$E(f ; \\mathcal{D})=\\int_{\\boldsymbol{x} \\sim \\mathcal{D}} \\mathbb{I}(f(\\boldsymbol{x}) \\neq y) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}$$\n",
    "\n",
    "#### 2.3.2 查全率、查准率与F1\n",
    "* 查准率、查全率\n",
    "错误率相当于把对的判错和错的判对一刀切了，不够精确，因此才产生了P(查准)、R(查全)\n",
    "  - 查准率：Precision，正例里边有多准，即有多少正例是真正例；\n",
    "  - 查全率：Recall，所有真实的正例，有多少被查出来了\n",
    "  - 由此有一个混淆矩阵：\n",
    "![confusion matrix](image/分类结果混淆矩阵.png)\n",
    "  - 对应地有：（还是尽量去理解，不要死记公式）\n",
    "$$P=\\frac{T P}{T P+F P}，R=\\frac{T P}{T P+F N}$$\n",
    "  - 查准率与查全率往往是一堆矛盾：想查的准就很难查的全，想查的全就很难查的准（就好像抓间谍一样）。\n",
    "* 查准查全的计算方式：\n",
    "  - 先排列，把最可能是正例的排在前面，最不可能的排到后面这样依次递减。\n",
    "  - 然后逐个地画线，没有正例、仅有1个正例、2个、3个……全部是正例，相当于有m个样例就计算m+1（注意0个）次查全、查准率，最后平滑地把他们连起来。所以查全、查准率也是步进计算的。\n",
    "  - 如果一个学习器的P-R曲线被另一个完全包住，那么后者的性能必然优于前者（相当于P、R双高）。\n",
    "* 平衡点、F1\n",
    "  - 平衡点（BEP, Break-Even Point）：查准率=查全率的取值。当两个学习器打的难舍难分的时候，往往看两者相等时的取值。\n",
    "  - F函数解决的是，**在学习器、PR曲线确定之后，怎么取阈值最好的问题**（也即取P、R曲线的哪个点）。$\\beta$的取值则考虑查准率和查全率之间的取舍，F函数使用加权调和平均：\n",
    "  $$\\frac{1}{F(\\beta)}=\\frac{1}{1+\\beta^{2}} \\cdot\\left(\\frac{1}{P}+\\frac{\\beta^{2}}{R}\\right)$$\n",
    "  化简后有：\n",
    "$$F(\\beta)=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R}$$\n",
    "  - $\\beta$反映了我们对于 查准率/查全率 这个比值的偏好程度。因为$\\beta$加在了调和平均R的头上，P、R都在分母上，所以如果$\\beta<1$则P越小越好、R越大越好、也即P/R越小越好；$\\beta>1$则P越小越好、R越大越好，也即P/R越小越好（但总之这些都是相对的，如果P、R都能提高那才是硬道理）。\n",
    "  - F函数中常取的是F1，就是纯调和平均：\n",
    "  $$\\frac{1}{F 1}=\\frac{1}{2} \\cdot\\left(\\frac{1}{P}+\\frac{1}{R}\\right)$$\n",
    "  化简后有：\n",
    "  $$F 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 \\times T P}{\\text { 样例总数 }+T P-T N}$$\n",
    "  - 往往随着阈值的变化，F函数的值也会先增再减，那么此时就取F值最大的那个阈值，就是最佳的预测模型。\n",
    "![confusion matrix](image/F1.gif)\n",
    "此外，关于P、R、F1的理解，可以参见这篇知乎：https://zhuanlan.zhihu.com/p/92218196 总之要动态地理解P、R、F1。\n",
    "\n",
    "#### 2.3.3 ROC与AUC\n",
    "与P-R曲线类似，ROC的操作过程也是一样的，只是ROC考虑的是TPR、FPR。\n",
    "* TPR、FPR\n",
    "全是从混淆矩阵竖着看的，即分母都是真实值的总和\n",
    "  - 真正例率（TPR，True Positive Rate）：**TPR的值和查全率是一样的！！！** 所有正例当中有多少被预测出来了，所以也被称作敏感度（sensitivity）。\n",
    "$$\\mathrm{TPR}=\\frac{T P}{T P+F N}$$\n",
    "  - 假正例率（FPR，False Positive Rate）：所有预测的正例当中，假正例在全部反例中的比例，因而wiki上也叫它“false alarm”。\n",
    "$$\\mathrm{FPR}=\\frac{F P}{T N+F P}$$\n",
    "  - P、R的分子一样、分母也有交集（实际上都是分子）；而TPR、FPR是完全没有交集。\n",
    "* ROC：受试工作者特征，Receiver Operating Characteristic\n",
    "  - 纵轴是TPR、横轴是FPR。\n",
    "![](image/ROC.png)\n",
    "  - ROC图也是步进的，不过这种步进就显得更有节奏了：\n",
    "<br>  随着阈值线不断向下，若当前为真正例，则y（TPR）步进$\\frac{1}{m^+}$；若当前为假正例，则x（FPR）步进$\\frac{1}{m^-}$。其中$m^+$、$m^-$分别对应正例和反例的总数（也即TPR、FPR的两个分母）。\n",
    "  - AUC即Area Under Curve，ROC曲线下的面积：AUC可以一定程度上衡量模型的好坏，AUC越大越好（显然，如果一个ROC曲线包住了另一条，那么前者性能必然优于后者）。\n",
    "  - 理解这一点可以从两个角度：\n",
    "<br>  ①定性地理解：我们希望TPR尽快地攀升、同时又希望FPR不要攀升得那么快（false alarm），如果用抓间谍的例子来理解的话就是，我们希望更快地把全部间谍都抓到（这对应TPR的攀升），又希望少错杀一些同志（FPR不要攀升那么快），所以这对应的确实就是AUC。\n",
    "<br>  ②定量地理解，引入$\\ell_{rank}$表示这种排序错乱的误差、损失、惩罚。\n",
    "$$\\ell_{r a n k}=\\frac{1}{m^{+} m^{-}} \\sum_{\\boldsymbol{x}^{+} \\in D^{+}} \\sum_{\\boldsymbol{x}^{-} \\in D^{-}}\\left(\\mathbb{I}\\left(f\\left(\\boldsymbol{x}^{+}\\right)<f\\left(\\boldsymbol{x}^{-}\\right)\\right)+\\frac{1}{2} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}^{+}\\right)=f\\left(\\boldsymbol{x}^{-}\\right)\\right)\\right)$$\n",
    "    那么有：$AUC=1-\\ell_{rank}$，也就是说其对应ROC之上的面积。那么要想误差越小，自然就要AUC越大。\n",
    "\n",
    "#### 2.3.4 代价敏感错误率与代价曲线\n",
    "目标：假正例（错的判对）与假反例（对的判错）的代价往往是不同的，比如对于患者的诊断。因而引入非均等代价。\n",
    "* 引入代价矩阵：（多维情况可进一步拓展）\n",
    "![](image/cm.png)\n",
    "  - 通常来讲，$cost_{ii}=0$。\n",
    "  - 故，对应地，“代价敏感”(cost-sensitive)错误率为：\n",
    "$$\n",
    "E(f ; D ; \\cos t)= \\frac{1}{m}\\left(\\sum_{\\boldsymbol{x}_{i} \\in D^{+}} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right) \\times cos t_{01}\\right.\n",
    "\\left.+\\sum_{\\boldsymbol{x}_{i} \\in D^{-}} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right) \\times cos t_{10}\\right)\n",
    "$$\n",
    "* 非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而代价曲线可达到该目的\n",
    "  - 计算代价曲线的步骤是：同样是步进划线，ROC曲线上的每一个点都对应代价曲线上的每一条直线\n",
    "  - 对于每一个ROC点，计算此时的FPR、FNR（毕竟代价曲线只考虑错误的代价），绘制(0, FPR)→(1,FNR)的许多条直线\n",
    "  - 此时的AUC即为期望总体代价\n",
    "![](image/代价曲线.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 假设检验\n",
    "总的来讲，此节是在讲如何通过实验来确定学习器的性能好坏。此时假设检验就是一个比较好的手段：即从概率上讲，A的泛化性能是否优于B，以及这个结论的把握有多大。\n",
    "目前理解的还不够，暂略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 偏差与方差\n",
    "除了通过实验可以从统计学角度比较两个学习器的泛化能力之外，人们还想知道“为什么”具有这样的泛化性能。“偏差-方差分解”(bias-variance decomposition)是一种重要工具。通过数学的推导，可以证明：\n",
    "$$E(f ; D)=\\operatorname{bias}^{2}(\\boldsymbol{x})+\\operatorname{var}(\\boldsymbol{x})+\\varepsilon^{2}$$\n",
    "也就是说，**泛化误差可分解为偏差、方差和噪声之和**。\n",
    "* 一般来讲，偏差与方差室友冲突的，这称为偏差-方差窘境\n",
    "* 对应地有如下曲线：对应着从欠拟合到过拟合\n",
    "![](image/泛化误差.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML C3 线性模型\n",
    "首先要把握的是本章的一个脉络：其实读了两三遍、查了很多资料之后就会发现周老师写这一章确实是很用心、很有想法的，写得确实好。\n",
    "<br>3.1节首先给出基本形式；3.2节先讨论如何在回归问题中建立线性模型（这也比较符合直觉）；3.3节从回归任务拓展到分类任务，并介绍了一种常见的二分类任务模型：对数几率回归；3.4节给出了另外一种二分类任务的处理方法，并且亦可拓展到多分类任务；3.5节最终将二分类任务拓展到多分类任务，即如何将多分类任务转化为二分类任务并解决之；最后3.6节给出了一些补充的关于类别不平衡问题的处理方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 基本形式\n",
    "事实上，线性模型在生活当中很常用，比如“三分天注定，七分靠打拼”这样的说法实际上就是一个线性模型。大概这也是线性模型“可理解性”的根基。\n",
    "* 直接给出多属性（多维）的线性模型：\n",
    "$$f(\\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{d} x_{d}+b$$\n",
    "* 或常用向量形式写成：\n",
    "$$f(\\boldsymbol{x})=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b$$\n",
    "* 所以整个线性模型的建立，其核心就在于找到一个合适的$\\boldsymbol{w}$和$b$（也就是所谓的线性拟合）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 线性回归\n",
    "对于回归任务，其输出是连续的（或者说是0到1之间的任意数），输入并不一定。通常可以将语言模糊化，比如输入高、中、低分别转化为{1, 0.5, 0}。\n",
    "* 先捏个软柿子，考虑仅有一个属性输入的情况（此时总维度就是二维的，对应一个平面），此时线性模型可表示为：\n",
    "$$f\\left(x_{i}\\right)=w x_{i}+b, \\text { 使得 } f\\left(x_{i}\\right) \\simeq y_{i}$$\n",
    "* 给一个例子可能会显得不那么抽象一些，考虑学生学习时长与成绩的关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#OrderedDict,实现了对字典对象中元素的排序。\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "#数据集\n",
    "examDict={\n",
    "    '学习时间':[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,\n",
    "            2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50],\n",
    "    '分数':[10,  22,  13,  43,  20,  22,  33,  50,  62,  \n",
    "              48,  55,  75,  62,  73,  81,  76,  64,  82,  90,  93]}\n",
    "examOrderDict=OrderedDict(examDict)\n",
    "examDf=pd.DataFrame(examOrderDict)\n",
    "\n",
    "#查看数据集前5行\n",
    "examDf.head()\n",
    "\n",
    "\n",
    "#提取特征和标签\n",
    "#特征features\n",
    "exam_X = examDf.loc[:,'学习时间']\n",
    "#标签labes\n",
    "exam_Y = examDf.loc[:,'分数']\n",
    "\n",
    "#绘制散点图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#散点图\n",
    "plt.scatter(exam_X, exam_Y, color=\"b\", label=\"exam data\")\n",
    "\n",
    "#添加图标标签\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Score\")\n",
    "#显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么如果我们尝试对这个数据集建立线性模型，无非就是找到一条直线能够很好地拟合这些点。\n",
    "* 这种拟合的依据有很多，其中比较常用的方式是高斯、勒让德等人提出的最小二乘法(least square error)，二乘就是平方的意思，台湾直接翻译成最小平方法，这种方式使用欧氏距离，通过平方的方式以消除负值带来的影响，那么也就是有：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(w^{*}, b^{*}\\right) &=\\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(x_{i}\\right)-y_{i}\\right)^{2} \\\\\n",
    "&=\\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "* 不适用欧氏距离拟合的也有，比如最小绝对值法：\n",
    "$$\\mathrm{MAE}=\\frac{\\sum_{i=1}^{n}\\left|y_{i}-\\hat{y}_{i}\\right|}{n}$$\n",
    "* 显然，最小二乘法的拟合是唯一的，拟合效果更好（最小绝对值法遇到0，2，0，2这种数据，就拟合到0-2之间任一个数都可以，而最小二乘法就只会给出1，这种约束性更强），但是最小二乘法对于异常值的扰动比较大。\n",
    "* 另一种Huber Loss结合了两者的优点，设定了一个阈值，在此阈值以下使用最小二乘，在此阈值以上使用绝对值法并减去一个平方量，这种方式也叫SMAE：\n",
    "$$\\mathrm{SMAE}_{i}=\\left\\{\\begin{array}{ll}\n",
    "\\frac{1}{2}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}, & \\text { when }\\left|y_{i}-\\hat{y}_{i}\\right|<\\delta, \\\\\n",
    "\\delta\\left|y_{i}-\\hat{y}_{i}\\right|-\\frac{1}{2} \\delta^{2}, & \\text { otherwise }\n",
    "\\end{array}\\right.，{\\mathrm{SMAE}}=\\frac{\\sum \\mathrm{SMAE}_{i}}{n}$$\n",
    "* 这些都是线性回归问题（甚至不止于此）当中的损失函数，像最小二乘也被称为L2损失，最小绝对值也被称为L1损失（对应他们的范数）。Huber损失虽然结合了两者的优点，但是引入了未知参数delta，故也需要额外的约束条件。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后我们考虑多元的线性回归问题，此时我们并不只输入一个属性，而是多个属性，对应地有：\n",
    "$$f\\left(\\boldsymbol{x}_{i}\\right)=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b, \\text { 使得 } f\\left(\\boldsymbol{x}_{i}\\right) \\simeq y_{i}$$\n",
    "考虑把b融合进w中，并将x和y都划为向量的形式有：\n",
    "$$\\hat{\\boldsymbol{y}}=\\mathbf{X} \\hat{\\boldsymbol{w}}$$\n",
    "其中，\n",
    "$$\\hat{\\boldsymbol{y}}=(\\hat{y_{1}} ; \\hat{y_{2}} ; \\ldots ; \\hat{y_{m}})$$\n",
    "$$\\mathbf{X}=\\left(\\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & \\dots & x_{1 d} & 1 \\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2 d} & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x_{m 1} & x_{m 2} & \\dots & x_{m d} & 1\n",
    "\\end{array}\\right)=\\left(\\begin{array}{ccc}\n",
    "x_{1}^{\\mathrm{T}} & 1 \\\\\n",
    "x_{2}^{\\mathrm{T}} & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m}^{\\mathrm{T}} & 1\n",
    "\\end{array}\\right)$$\n",
    "$$\\hat{\\boldsymbol{w}}=(\\boldsymbol{w} ; b)$$\n",
    "利用最小二乘法可以得到：\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n",
    "关于此式为什么是最小二乘法，可以逐一解读如下图：\n",
    "![](image/lse解读.png)\n",
    "对此式进行求导后可以得到最小二乘法的**正规方程(normal equation)**：\n",
    "$$\\hat{\\boldsymbol{w}}^{*}=\\left(\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\mathrm{T}} \\boldsymbol{y}$$\n",
    "那么就有必要讨论这个$X^{-1}X$是不是可逆的了。然而很不幸，在现实情况中，往往不可逆，如果样本比属性多，那么我们倒还可以砍掉一些样本，但如果要确定的属性比样本还要多，那么就不可逆需要进一步处理了。\n",
    "<br>事实上这也对应着前面讲到的归纳偏好问题，常见的方式是通过正则化处理。\n",
    "* 再进一步推广，可以得到广义线性模型。即函数在某一更广泛的尺度上变化，但我们可以将之转化为线性模型处理，也即：\n",
    "$$y=g^{-1}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right)$$\n",
    "这里的g(·)被称为“联系函数”(link function)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 对数几率回归\n",
    "这一节主要讲如何用回归模型解决分类问题，即通过一种转化的方式，将回归的预测值转化为分类。\n",
    "* 最简单、直观的当然是单位阶跃函数（也叫Heaviside函数）：\n",
    "$$y=\\left\\{\\begin{array}{cc}\n",
    "0, & z<0 \\\\\n",
    "0,5, & z=0 \\\\\n",
    "1, & z>0\n",
    "\\end{array}\\right.$$\n",
    "但是这个函数问题在于，螃蟹虽然好吃，可没有第一个吃螃蟹的人。这个函数不连续，无法求导，也就无法确定w和b的取值，也就是说我们事实上学习不出来wx+b。\n",
    "* 于是对数几率函数就成为一个很好的选择（也是Sigmoid函数的一种）：\n",
    "$$y=\\frac{1}{1+e^{-z}}$$\n",
    "之所以叫对数几率是因为，如果我们对它进行数学处理，就会得到\n",
    "$$\\ln \\frac{y}{1-y}=w^{\\mathrm{T}} x+b$$\n",
    "而y/1-y恰反映了整理可能性与反例可能性的比值，所以叫“对数几率”。\n",
    "* 那么接下来我们就需要确定w和b：这里区别于回归任务中使用最小二乘法，分类任务（显然不再适合最小二乘）可以引入概率的方式来确定w和b，将y和1-y对应地修改为$p(y=1 \\mid \\boldsymbol{x})$和$p(y=0 \\mid \\boldsymbol{x})$，显然这两者非黑即白，两者总和为1。\n",
    "* 于是可以进一步进行数学处理，使用最大似然法得到取对数的最大似然函数：\n",
    "$$\\ell(\\boldsymbol{w}, b)=\\sum_{i=1}^{m} \\ln p\\left(y_{i} \\mid \\boldsymbol{x}_{i} ; \\boldsymbol{w}, b\\right)$$\n",
    "进一步变形：\n",
    "![](image/进一步变形.png)\n",
    "最终得到：\n",
    "$$\\boldsymbol{\\beta}^{*}=\\underset{\\boldsymbol{\\beta}}{\\arg \\min } \\ell^{\\prime}(\\boldsymbol{\\beta})\n",
    "，\\ell^{\\prime}(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(-y_{i} \\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}+\\ln \\left(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}}\\right)\\right)$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 线性判别分析\n",
    "引入：仍然考虑一个二分类问题。\n",
    "<br>在一个三维空间中，有一张桌子，其上放着一个苹果和一个橙子。\n",
    "<br>已知苹果和橙子的点集（样本集），输入一个测试点（要么是苹果要么是橙子），如何判断其为苹果还是橙子？\n",
    "![](image/苹果橙子.jpg)\n",
    "而我们用这个视角去对苹果和橙子照相实际上就是一个将三维投降二维的过程，所以LDA的思想其实非常简单、易用：设法将样例投影到一条直线上，使得同类样例的投影点尽可能近、异类样例的投影点尽可能远离（对于更高维度，那么就是投影到低维超平面上）。\n",
    "![](image/LDA.png)\n",
    "* 二维LDA\n",
    "<br>于是，得到这个二分类问题均值的投影和方差分别为：$\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\mu}_{0}, \\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\mu}_{1}, \\boldsymbol{w}^{T} \\boldsymbol{\\Sigma}_{0} \\boldsymbol{w} \\text { 和 } \\boldsymbol{w}^{T} \\boldsymbol{\\Sigma}_{1} \\boldsymbol{w}$\n",
    "同样地，我们需要找到一个理论依据去确定w，这里做除法可以引入广义瑞利商以确定w：\n",
    "$$\\begin{aligned}\n",
    "J &=\\frac{\\left\\|\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\mu}_{0}-\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\mu}_{1}\\right\\|_{2}^{2}}{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\Sigma}_{0} \\boldsymbol{w}+\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{\\Sigma}_{1} \\boldsymbol{w}} \\\\\n",
    "&=\\frac{\\boldsymbol{w}^{\\mathrm{T}}\\left(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1}\\right)\\left(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{w}}{\\boldsymbol{w}^{\\mathrm{T}}\\left(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1}\\right) \\boldsymbol{w}}\n",
    "\\end{aligned}$$\n",
    "分别定义类内散度矩阵和类间散度矩阵，可以得到广义瑞利商形式的损失函数，我们希望J越大越好：\n",
    "$$J =\\frac{\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b} \\boldsymbol{w}}{\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{w} \\boldsymbol{w}}$$\n",
    "根据瑞利商的性质（或使用拉格朗日乘子法推导），可以最终得到：\n",
    "$$\\boldsymbol{w}=\\mathbf{S}_{w}^{-1}\\left(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1}\\right)$$\n",
    "于是又变成了求逆问题的讨论。这里书上给出的是SVD分解（然而为什么不用LU分解等更多方阵分解方式呢？）。\n",
    "* 多维LDA\n",
    "在多维任务中，需要引入全局散度矩阵，将该类别的个数作为类间散度矩阵的权重，有：\n",
    "$$\\mathbf{S}_{w}=\\sum_{i=1}^{N} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\mathrm{T}}$$\n",
    "$$\\mathbf{S}_{b}=\\mathbf{S}_{t}-\\mathbf{S}_{w}=\\sum_{i=1}^{N} m_{i}\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}$$\n",
    "并且常用迹作为目标函数的分子、分母：\n",
    "$$\\mathbf{W}=\\max _{\\mathbf{W}} \\frac{\\operatorname{tr}\\left(\\mathbf{W}^{\\mathrm{T}} \\mathbf{S}_{b} \\mathbf{W}\\right)}{\\operatorname{tr}\\left(\\mathbf{W}^{\\mathrm{T}} \\mathbf{S}_{w} \\mathbf{W}\\right)}$$\n",
    "这里W又称投影矩阵。因此LDA也常用于将多维降维的过程中，被视为一种经典的监督降维技术。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 多分类学习\n",
    "这同样是一种Redution的思想：将多分类学习转化为二分类问题并解决之。关于此部分可以见slides中的四大天王的例子更形象地理解。\n",
    "* OvO、OvR和MvM\n",
    "  - OvO：会产生$C_{n}^{2}=N(N-1) / 2$个二分类任务，N个类别两两配对：比如四种分类就对应6个任务。\n",
    "  - OvR：亦称OvA，一个为正类，其余全为反类\n",
    "  - MvM：通过某些特殊的设计构造正、反类（比如ECOC）。显然OvO和OvR都是MvM的特殊情况。\n",
    "* OvO vs OvR\n",
    "  - OvO的存储开销和测试时间开销通常比OvR大，但是在类别较多时训练时间开销通常更小；\n",
    "  - OvO在训练时，每个分类器仅用到部分样例，OvR则为全部样例；\n",
    "  - 多数情况下，两者预测性能差不多；\n",
    "* ECOC编码长度\n",
    "  - ECOC编码对分类器错误有一定的容忍和修正能力；\n",
    "  - ECOC编码越长，纠错能力越强，但也意味着所需训练的分类器越多，计算、存储开销都会增大；\n",
    "  - 组合数目也是有限的，码长超过一定范围后就失去了意义；\n",
    "  - 通常，机器学习涉及很多因素，并不是说编码理论性质越好，分类性能越好。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 类别不平衡问题\n",
    "类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况。\n",
    "<br>比如对学习器输入了998份胃癌患者样例、2份非胃癌患者样例，那么如果我们不加以“再缩放”(rescaling)的话，学习器在拿到大部分新样本时，很可能都判以患者。所以rescaling解决的主要是“训练集未必是真实样本总体的无偏采样”的问题，很多时候我们也并不知道样本是不是无偏的。\n",
    "* 再缩放：以对数几率模型为例\n",
    "在对数几率模型中我们有：\n",
    "$$\\frac{p(y=1 \\mid \\boldsymbol{x})}{p(y=0 \\mid \\boldsymbol{x})}=e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b}$$\n",
    "$$\\begin{aligned}\n",
    "\\frac{y^{\\prime}}{1-y^{\\prime}} &=\\frac{p(y=1 \\mid \\boldsymbol{x})}{p(y=0 \\mid \\boldsymbol{x})} \\times \\frac{m^{-}}{m^{+}}=e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b} \\times \\frac{m^{-}}{m^{+}} \\\\\n",
    "&=e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b} \\times e^{\\ln \\frac{m^{-}}{m^{+}}}=e^{\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b+\\ln \\frac{m^{-}}{m^{+}}}\n",
    "\\end{aligned}$$\n",
    "所以实际上，这个操作是在线性模型中附加了一个截距因子\n",
    "* 对于类别不平衡问题，一般有三类做法\n",
    "  - 欠采样(undersampling，也叫下采样)，去除一些反例使得正、反例数目接近，再学习。→时间开销较小，“全局来看不会丢失重要信息”，代表如EasyEnsemble；\n",
    "  - 过采样(oversampling，也叫上采样)，增加一些正例使得正、反例数目接近，再学习。→不能简单地对初始样本重复采样，否则过拟合，可以通过插值，代表如SMOTE；\n",
    "  - 阈值移动(threshold-moving)，即采用再缩放的基本策略。→也是代价敏感学习的基础。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
